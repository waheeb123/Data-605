---
title: "Homework-12"
author: "waheeb Algabri"
output:
  html_document:
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
```
```{r, message = FALSE}
library(tidyverse)
library(cowplot)
library(car)
library(jtools) # For tabulating and visualizing results from multiple regression models
library(corrplot)
```

# Read in Data

| Variable        | Definition                                                               |
|-----------------|--------------------------------------------------------------------------|
| Country         | Name of the country                                                       |
| LifeExp         | Average life expectancy for the country in years                          |
| InfantSurvival  | Proportion of those surviving to one year or more                         |
| Under5Survival  | Proportion of those surviving to five years or more                       |
| TBFree          | Proportion of the population without TB                                   |
| PropMD          | Proportion of the population who are MDs                                  |
| PropRN          | Proportion of the population who are RNs                                  |
| PersExp         | Mean personal expenditures on healthcare in US dollars at average exchange rate |
| GovtExp         | Mean government expenditures per capita on healthcare in US dollars at average exchange rate |
| TotExp          | Sum of personal and government expenditures                               |


```{r , results='hide'}
 df<- read_csv("https://raw.githubusercontent.com/waheeb123/Datasets/main/real-world%20data%20from%202008")
```

```{r}
glimpse(df)
```



# Task 1

Provide a scatterplot of LifeExp~TotExp, and run simple linear regression. Do not transform the  variables. Provide and interpret the F statistics, R^2, standard error,and p-values only. Discuss whether the assumptions of simple linear regression met.

### Scatterplot

```{r}
df %>%
  ggplot(aes(TotExp, LifeExp)) +
  geom_point()
```

This scatter plot does not indicate a clear linear relationship. There does seem to be *some* kind of relationship, however, as the high values of Total Expenditures appear correlated with high values of Life Expectancy.

### Model

```{r}
model1 <- lm(LifeExp ~ TotExp, data = df)
export_summs(model1)
```

```{r}
summary(model1)
```


Based on this summary alone, the model appears half-way decent. The F-stat indicates the model is significant, and the sole predictor appears significant as well, given that the p-values of both approach zero. The $R^2$ is quite low, however, indicating that Total Expenditures only explains ~25% of the variance in Life Expectancy. The standard error is relatively low, too, at about 1/10th the value of the $\beta$ coefficient.

```{r}
ctrd <- cor(df[, sapply(df, is.numeric)])
corrplot(ctrd
         , method = 'color' # I also like pie and ellipse
         , order = 'hclust' # Orders the variables so that ones that behave similarly are placed next to each other
         , addCoef.col = 'black'
         , number.cex = .6 # Lower values decrease the size of the numbers in the cells
         )

```


```{r}
results <- summary(model1)
results$coefficients[2, 2] / results$coefficients[2, 1]

```

### Residuals and Assumptions

When we look at the residuals, however, the issues resurface. They display non-constant variance and appear non-normally distributed. These indicate the our predictor and response variables do not share a linear relationship. The assumptions of OLS appear violated.

```{r}
par(mfrow = c(2, 2), mar = c(2,2,2,2))
plot(model1)
```

# Task 2

Raise life expectancy to the 4.6 power (i.e., LifeExp^4.6). Raise total expenditures to the 0.06 power (nearly a log transform, TotExp^.06). Plot LifeExp^4.6 as a function of TotExp^.06, and re-run the simple regression model using the transformed variables. Provide and interpret the F statistics, R^2, standard error, and p-values. Which model is "better?"

### Plot

```{r}
df <- df %>%
  mutate(
    LifeExp_powerT = LifeExp^4.6,
    TotExp_powerT = TotExp^0.06
  )

df %>%
  ggplot(aes(LifeExp_powerT, TotExp_powerT)) +
  geom_point()
```

Already, things look better. There is now a clear linear relationship!

### Model

```{r}
model1 <- lm(LifeExp_powerT ~ TotExp_powerT, data = df)
summary(model1)
```

The overall fit and the TotExp variable appear significant, with very low p-values. This time, however, our $R^2$ is much higher at ~72%, indicating our transformed predictor explains much more of the transformed response's variance. 

```{r}
results <- summary(model1)
results$coefficients[2,2] / results$coefficients[2,1]
```

Our standard error is also much smaller.

### Residuals and Assumptions

```{r}
par(mfrow = c(2, 2), mar = c(2,2,2,2))
plot(model1)
```

Our residuals are also different. They appear much more uniformly scattered and normally distributed (despite some left tail outliers). This model most certainly provides a better fit.

```{r}
powerTransform(LifeExp ~ TotExp, data = df)
```
# Task 3

Using the results from 3, forecast life expectancy when TotExp^.06 = 1.5. Then forecast life 
expectancy when TotExp^.06 = 2.5.

```{r}
prediction1 <- predict(model1, newdata = data.frame(TotExp_powerT = 1.5))^(1/4.6)
prediction2 <- predict(model1, newdata = data.frame(TotExp_powerT = 2.5))^(1/4.6)

cat(
  'Prediction with 1.5: ',
  scales::comma(prediction1),
  '\nPrediction with 2.5: ',
  scales::comma(prediction2),
  sep = ''
)
```

We must undo the transformation we applied to the response variable when forming our predictions. With that, we get reasonable predictions.

# Task 4

Build the following multiple regression model and interpret the F Statistics, R^2, standard error, 
and p-values. How good is the model?

$$LifeExp =\beta_0 + \beta_1 PropMd + \beta_1 TotExp + \beta_1 PropMD \times TotExp$$

```{r}
model1 <- lm(LifeExp ~ PropMD + TotExp + PropMD:TotExp, data = df)
results <- summary(model1)
print(results)
cat(
  '--Standard Error to Coefficient Ratios--\n',
  'PropMD: ',abs(results$coefficients[2,2] / results$coefficients[2,1]),'\n',
  'TotExp: ',abs(results$coefficients[3,2] / results$coefficients[3,1]),'\n',
  'Interaction: ',abs(results$coefficients[4,2] / results$coefficients[4,1]),'\n',
  '--------\n', 
  sep=''
)

par(mfrow = c(2,2), mar = c(2,2,2,2))
plot(model1)
```

We get similar results as we saw in our initial model fit. The F-test and predictor t-test p-values indicate the overall model and all predictors are significant. Our $R^2$ is relatively low, explaining only ~35% of the response variance. Standard errors are relatively higher, all above 10% of the relevant coefficient. And finally, residuals again appear non-normal and heterskedastic.

# Task 5

Forecast LifeExp when PropMD = 0.03 and TotExp = 14. Does this forecast seem realistic? Why or why not?

```{r}
newdata = data.frame(PropMD = 0.03, TotExp = 14)
predict(model1, newdata = newdata)
```

A predicted Life Expectancy of ~107.7 years does not seem reasonable. First, the maximum life expectancy in the whole dataset is 83. Second, a Total Expenditure of 14 is quite low, as seen in the plot below, so a prediction of very high life expectancy defies expectations. A Proportion of MDs of 0.3 is, on the other hand, quite high, but not high enough to warrant a life expectancy of 107 years. The country with the highest PropMD in the dataset (San Marino with ~0.35) only has a Life Expectancy of 82 years.

```{r}
max(df$LifeExp)

ggplot(df, aes(PropMD)) +
  geom_histogram(bins = 20) +
  geom_vline(xintercept = mean(df$PropMD), color = 'red')

ggplot(df, aes(TotExp)) +
  geom_histogram(bins = 20) +
  geom_vline(xintercept = mean(df$TotExp), color = 'red')

df %>%
  filter(PropMD == max(df$PropMD))
```

So, it seems this unrealistic prediction is an indication of problems with the model. This point aligns with the residual problems and sizeable standard errors we identified previously. Let's see if we can apply some transformations to improve the fit and produce a better prediction.

```{r}
y_transformation <- powerTransform(model1)
y_transformed <- bcPower(df$LifeExp, y_transformation$lambda)

x1_transformation <- powerTransform(PropMD ~ 1, data = df)
x1_transformed <- bcPower(df$PropMD, x1_transformation$lambda)

x2_transformation <- powerTransform(TotExp ~ 1, data = df)
x2_transformed <- bcPower(df$TotExp, x1_transformation$lambda)

df <- df %>%
  mutate(
    LifeExp_powerT = y_transformed,
    PropMD_powerT = x1_transformed,
    TotExp_powerT = x2_transformed
  )

df %>%
  ggplot(aes(PropMD_powerT, LifeExp_powerT)) +
  geom_point()

df %>%
  ggplot(aes(TotExp_powerT, LifeExp_powerT)) +
  geom_point()

model1 <- lm(
  LifeExp_powerT ~ PropMD_powerT + TotExp_powerT + PropMD_powerT:TotExp_powerT, 
  data = df
)
print(summary(model1))

par(mfrow = c(2,2), mar = c(2,2,2,2))
plot(model1)

newdata = data.frame(
  PropMD_powerT = 0.03^x1_transformation$lambda,
  TotExp_powerT = 14^x2_transformation$lambda
)
predict(model1, newdata = newdata)^(1/y_transformation$lambda)
```

After applying transformations, our model fit appears much better. Residuals are roughly normal with reasonable constant variance, and the $R^2$ is much higher. Our prediction is also much more reasonable. It's very low, but not outside what is seen in the dataset (e.g. Sierra Leone has a Life Expectancy of only 40 years). These transformations appear very effective!

